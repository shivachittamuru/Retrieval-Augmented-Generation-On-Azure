{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azure-search-documents\n",
      "Version: 11.4.0a20230509004\n",
      "Summary: Microsoft Azure Cognitive Search Client Library for Python\n",
      "Home-page: https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents\n",
      "Author: Microsoft Corporation\n",
      "Author-email: ascl@microsoft.com\n",
      "License: MIT License\n",
      "Location: C:\\Users\\shchitt\\AppData\\Local\\anaconda3\\envs\\aoai\\Lib\\site-packages\n",
      "Requires: azure-common, azure-core, isodate\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Private Preview edition\n",
    "# ! pip install --index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ azure-search-documents==11.4.0a20230509004\n",
    "! pip show azure-search-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the API keys in environment variables\n",
    "import os\n",
    "import os, json\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model = os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_endpoint: str = os.environ['AZURE_COGNITIVE_SEARCH_ENDPOINT']\n",
    "vector_store_password: str = os.environ['AZURE_COGNITIVE_SEARCH_KEY']\n",
    "index_name: str = \"langchain-rag-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.embeddings.openai.OpenAIEmbeddings"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(model=embedding_model, chunk_size=1)\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_endpoint,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"files/state_of_the_union.txt\")\n",
    "loader = TextLoader(\"files/state_of_the_union.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nzk5MWE4NGUtYTQzMy00ZmY5LThlZTgtOGQ2Y2Y1Zjk1ZWFj',\n",
       " 'MDhjM2ExNjUtN2M3Yy00YzU0LWE2ZWUtYTVhNDM3NTg3M2Y2',\n",
       " 'ODI2YzFhOTktYjI5Yi00YTM4LTk0ZjQtY2QyY2E2MjgyZjZj',\n",
       " 'MDk4MGYwZjktNWQ1MC00YzJlLWI2ZTYtMTEwNjBjNjE5MzUy',\n",
       " 'NGZmNjU0NTUtYjQwZS00YzZlLThiOTMtN2I1OTA0NTA0ZGNk',\n",
       " 'YmQ3MDBhMzgtMjk2Yi00ZjAzLTlkMjItYjAzYTBjMWIyNzIx',\n",
       " 'MTM5ZTQwMDItMTEyZC00MTRkLWJmYmItM2VjYjg5NWI2NmY1',\n",
       " 'ZmYxM2FkNjgtYzhhMi00YTMyLWE3ZTQtODc2ZTZhNGExYjc0',\n",
       " 'YjY2OTY1NzAtNzMzNC00NmJiLTkyNzEtNDhmM2Y0OGE4NTA2',\n",
       " 'YWM5YWYzYzMtYTY1NS00YTcyLWE4ZWEtMzFiNjEwOTY0Mzkz',\n",
       " 'YTkxOTBlZjItMzg2Mi00MTI4LWE5MzctOWQ4ZjI5YzFjNmZk',\n",
       " 'NGZiNzU0YjMtZDFmMS00YzVmLWFiNGEtNWE0ZjI0MTdhYTQ1',\n",
       " 'ODFkODVmMmYtZTAzMC00YjM3LWJlZTUtN2UzMmQzZGVlZmJm',\n",
       " 'NzllNDNlNjYtMjU5NC00MTg4LWFjZGYtMzk3NTQ3N2Y3MThl',\n",
       " 'YzNlYTBkZTQtNjA0Ny00NTZiLThiYmUtNGQ3Mjk4MjdhYmRm',\n",
       " 'MmVhYzg5NDktYzQyZi00ZmUxLThiMjMtMmY0YjdhMjNmM2Rh',\n",
       " 'NDY1ZjhmYTUtZjQwOC00MWNhLWIwOTgtNWUzNGYwYWJhOTE0',\n",
       " 'NzA1ZDA5YTYtZDEyZS00YmFmLWI4YzEtYjc1NGI5ZGI0ZmUx',\n",
       " 'MjQwYWQyMDctMzU0MC00NzA1LThjNjYtY2NjNWZlYmQxZTVl',\n",
       " 'ZGQwZTU1MjUtZDc0OS00NzRlLWE3OGQtM2IyNGMxYmVlM2Ew',\n",
       " 'NmEwNTY3N2ItZmQyOS00NGM5LWE4MWQtNTliZTVjYmI1ODAy',\n",
       " 'ZGViZWM5MjYtMWRlNy00ZDgzLTk1ZDktYmQ0ZWUwNTRjN2Ey',\n",
       " 'YmRkZjJjYjItYWExZC00ZTU1LWJiYzQtMzZkNDIzN2IwNzFj',\n",
       " 'MjhjZWViZWEtNTMxNC00YjJlLTgxNDktZWUxOWJjZTUyMjBh',\n",
       " 'ZGY2YmEzMzktYzMwMi00YzE0LWI1NmEtNWQ1MzFkOWQzMTg2',\n",
       " 'Mzg3ZDk5ZmQtMWZjNC00MmU1LThhN2ItYmVmMjI2NjkxNDlm',\n",
       " 'OGUyMTRlOTItOWI0MS00N2M1LWI0OWEtMGZlMGZmNzdkOGRi',\n",
       " 'OWJmODY5OWEtYWJlNi00ZjRmLTkyYzMtNjRmMTZhOTEzMzY3',\n",
       " 'ODkzNWEwMzYtNjkxZS00NTk1LWE4MDEtY2E2N2FhNTUxYzM5',\n",
       " 'NDY2ZWQ1MWQtN2QyMy00NWE3LTlmYWUtMDJkNzZhYWY2NDg4',\n",
       " 'ZDA1Mjg2MmUtNWM2Mi00MjFjLWEwYTYtZWJiMGJlN2JhMTY5',\n",
       " 'ZTU0ODcwZjItZmY2OC00OGZiLWJhNmEtOWI5OGY5ODRmNzFk',\n",
       " 'ZGE0NTQ3MmUtMWQ3MC00ZDVlLThmNDktMjMyZTRiNWJlYTUx',\n",
       " 'YjViMmE3ZGMtMTViOC00MTgwLWI3OGYtYTYwM2E5ZGZiYzVk',\n",
       " 'YjU0Nzg1YzgtYWExNC00NmI4LTk4NWQtZWZlNzM0NmIyMDE1',\n",
       " 'YTY3YWMyNjItMjhiMi00NzRiLWJlMTctMzE4ZjM5N2RhYzIy',\n",
       " 'ZGViNTc1MWYtYzdhZS00YjA1LWI4MDktOTFlYmI5MDRjOWYw',\n",
       " 'ZDdmZDZjM2UtY2QwNC00NTVkLWFjMDctMWVmYzA0ZmM2YjQw',\n",
       " 'NDZjMTljYjktOTgxMC00MzlmLTk2MzctYzUzZTFlYmNmNjdh',\n",
       " 'ZGE3ODRiYjAtMjVhOS00MDYwLWE5NjktYzkxZjk3OGJmNzdh',\n",
       " 'OTJhZTYyZTItZWZmNy00NGQyLTg4ZjMtMzZlMTExZmZkN2E1',\n",
       " 'ZTFlNDFhOWEtYjM4OS00ZWMzLWJiYWYtZDExZjMxZjM1MzE4']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = AzureChatOpenAI(temperature=0.0,\n",
    "    max_tokens=500,\n",
    "    openai_api_base=openai.api_base,\n",
    "    openai_api_version=openai.api_version,\n",
    "    deployment_name=chat_model,\n",
    "    openai_api_key=openai.api_key,\n",
    "    openai_api_type = openai.api_type    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try without using memory first! \n",
    "\n",
    "We can use RetrievalQA for that as it only focuses on \"query\" and \"context\" and does not require \"chat_history\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with your docs!\n",
      "User:\n",
      "What did the president say about Ketanji Brown Jackson?\n",
      "AI:\n",
      "The President nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court, and he described her as one of the nation's top legal minds who will continue Justice Breyer's legacy of excellence.\n",
      "User:\n",
      "Did he mention who she succeeded?\n",
      "AI:\n",
      "No, he did not mention who she succeeded.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "retrieval_chain = RetrievalQA.from_chain_type(llm=chat, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "print(\"Chat with your docs!\")\n",
    "while True:\n",
    "    query = input()\n",
    "    if query in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "    result = retrieval_chain({\"query\": query})\n",
    "    print(\"User:\")\n",
    "    print(result['query'])\n",
    "    print(\"AI:\")\n",
    "    print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with your docs!\n",
      "User:\n",
      "What did the president say about Ketanji Brown Jackson?\n",
      "AI:\n",
      "The President nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court, and he described her as one of the nation's top legal minds who will continue Justice Breyer's legacy of excellence.\n",
      "User:\n",
      "Did he mention who she succeeded?\n",
      "AI:\n",
      "The President mentioned Justice Stephen Breyer as the predecessor of Ketanji Brown Jackson.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Chat with your docs!\")\n",
    "while True:\n",
    "    query = input()\n",
    "    result = qa_chain({\"question\": query})\n",
    "    if query in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "    print(\"User:\")\n",
    "    print(result[\"question\"])\n",
    "    print(\"AI:\")\n",
    "    print(result[\"answer\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
