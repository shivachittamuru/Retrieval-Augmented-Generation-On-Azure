{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, BlobSasPermissions, generate_blob_sas\n",
    "import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload raw data (research papers) from \"data/unstructured/raw\" folder to an Azure Blob Storage container \"raw-research-papers\" before running this notebook. That will be your data store for your source data.\n",
    "\n",
    "Azure Forms Recognizer will be used to extract text and tables from research papers. \n",
    "\n",
    "Azure Cognitive Search is used to index the extracted JSON documents, and retreive relevant information using semantic search.\n",
    "\n",
    "Azure OpenAI service is used to generate the answers and summaries based on user prompts.\n",
    "\n",
    "Add necessary credentials of your Azure Resources in .env file before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_url(document_url):\n",
    "    \"\"\"Returns the text content of the file at the given URL.\"\"\"\n",
    "    #print(\"Analyzing\", document_url)\n",
    "    \n",
    "    poller = document_analysis_client.begin_analyze_document_from_url(\"prebuilt-layout\", document_url)\n",
    "    result = poller.result()\n",
    "    return result\n",
    "\n",
    "def get_page_content(result):\n",
    "    page_content = []\n",
    "    for page in result.pages:\n",
    "        all_lines_content = []\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            all_lines_content.append(' '.join([word.content for word in line.get_words()]))\n",
    "        page_content.append({'page_number':page.page_number, \n",
    "                                'page_content':' '.join(all_lines_content)})\n",
    "    return page_content\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authenticated_urls(container_name):\n",
    "    \"\"\"Returns a list of tuple of (document name, authenticated URLs) for\n",
    "    documents in the given container.\"\"\"\n",
    "\n",
    "    urls = []\n",
    "    # Connect to the storage account\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(os.environ['AZURE_BLOB_STORAGE_CONNECTION_STRING'])\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # Iterate over the blobs in the container\n",
    "    blob_list = container_client.list_blobs()\n",
    "    for blob in blob_list:\n",
    "        # Retrieve the URL of the blob\n",
    "        blob_client = container_client.get_blob_client(blob.name)\n",
    "        blob_url = blob_client.url\n",
    "\n",
    "        #print(f\"Generating authenticated URL for: {blob.name}\")\n",
    "\n",
    "        blob_sas = generate_blob_sas(\n",
    "            account_name=container_client.account_name,\n",
    "            account_key=container_client.credential.account_key,\n",
    "            container_name=container_name,\n",
    "            blob_name=blob.name,\n",
    "            permission=BlobSasPermissions(read=True),\n",
    "            expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=1))\n",
    "\n",
    "        authenticated_url = f\"{blob_url}?{blob_sas}\"\n",
    "        urls.append((blob.name, authenticated_url))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def table_to_html(table):\n",
    "    table_html = \"<table>\"\n",
    "    rows = [sorted([cell for cell in table.cells if cell.row_index == i], key=lambda cell: cell.column_index) for i in range(table.row_count)]\n",
    "    for row_cells in rows:\n",
    "        table_html += \"<tr>\"\n",
    "        for cell in row_cells:\n",
    "            tag = \"th\" if (cell.kind == \"columnHeader\" or cell.kind == \"rowHeader\") else \"td\"\n",
    "            cell_spans = \"\"\n",
    "            if cell.column_span > 1: cell_spans += f\" colSpan={cell.column_span}\"\n",
    "            if cell.row_span > 1: cell_spans += f\" rowSpan={cell.row_span}\"\n",
    "            table_html += f\"<{tag}{cell_spans}>{html.escape(cell.content)}</{tag}>\"\n",
    "        table_html +=\"</tr>\"\n",
    "    table_html += \"</table>\"\n",
    "    return table_html\n",
    "\n",
    "def get_document_text(result):\n",
    "    offset = 0\n",
    "    page_map = []\n",
    "\n",
    "    for page_num, page in enumerate(result.pages):\n",
    "        tables_on_page = [table for table in result.tables if table.bounding_regions[0].page_number == page_num + 1]\n",
    "\n",
    "        # mark all positions of the table spans in the page\n",
    "        page_offset = page.spans[0].offset\n",
    "        page_length = page.spans[0].length\n",
    "        table_chars = [-1]*page_length\n",
    "        for table_id, table in enumerate(tables_on_page):\n",
    "            for span in table.spans:\n",
    "                # replace all table spans with \"table_id\" in table_chars array\n",
    "                for i in range(span.length):\n",
    "                    idx = span.offset - page_offset + i\n",
    "                    if idx >=0 and idx < page_length:\n",
    "                        table_chars[idx] = table_id\n",
    "\n",
    "        # build page text by replacing charcters in table spans with table html\n",
    "        page_text = \"\"\n",
    "        added_tables = set()\n",
    "        for idx, table_id in enumerate(table_chars):\n",
    "            if table_id == -1:\n",
    "                page_text += result.content[page_offset + idx]\n",
    "            elif not table_id in added_tables:\n",
    "                page_text += table_to_html(tables_on_page[table_id])\n",
    "                added_tables.add(table_id)\n",
    "\n",
    "        page_text += \" \"\n",
    "        #page_map.append((page_num, offset, page_text))\n",
    "        page_map.append({'page_number':page_num, \n",
    "                         'offset':offset, \n",
    "                         'page_text':page_text}) \n",
    "        offset += len(page_text)\n",
    "        \n",
    "    return page_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_container_name = 'raw-research-papers'\n",
    "extracted_container_name = 'extracted-research-papers'\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(os.environ['AZURE_BLOB_STORAGE_CONNECTION_STRING'])\n",
    "extracted_container_client = blob_service_client.get_container_client(container=extracted_container_name)\n",
    "    \n",
    "if not extracted_container_client.exists():\n",
    "    extracted_container_client.create_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded extracted content for: AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf\n",
      "\n",
      "Uploaded extracted content for: Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf\n",
      "\n",
      "Uploaded extracted content for: Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf\n",
      "\n",
      "Uploaded extracted content for: LLMs_are_Human-Level_Prompt_Engineers.pdf\n",
      "\n",
      "Uploaded extracted content for: Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf\n",
      "\n",
      "Uploaded extracted content for: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf\n",
      "\n",
      "Uploaded extracted content for: Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf\n",
      "\n",
      "Uploaded extracted content for: Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_urls = get_authenticated_urls(raw_container_name)\n",
    "\n",
    "documents = []\n",
    "for document_name, document_url in document_urls:\n",
    "    result = extract_content_from_url(document_url)\n",
    "    page_map = get_document_text(result)\n",
    "    doc = {'filename':document_name, 'file_path':document_url, 'content':page_map}\n",
    "    \n",
    "    documents.extend(\n",
    "        [\n",
    "            {\n",
    "                'id': document_name.split('.')[0] + '-' + str(page['page_number']),\n",
    "                'file_name': document_name,\n",
    "                'file_path': document_url,\n",
    "                'page_number': page['page_number'],\n",
    "                'page_text': page['page_text']\n",
    "            }\n",
    "            for page in doc['content']\n",
    "        ]\n",
    "    )\n",
    "                                \n",
    "    blob_client = extracted_container_client.get_blob_client(blob=document_name[:-3] +'json')\n",
    "    blob_client.upload_blob(json.dumps(doc), overwrite=True)\n",
    "    print(f\"Uploaded extracted content for: {document_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'AutoPrompt_Eliciting_Knowledge_From_LanguageModels-5',\n",
       " 'file_name': 'AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf',\n",
       " 'file_path': 'https://shivaaistorage.blob.core.windows.net/raw-research-papers/AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf?se=2023-06-18T21%3A14%3A18Z&sp=r&sv=2022-11-02&sr=b&sig=M2u0l3Kq47hOcAObXsM7%2B05DV5Drfo957hyGoYpFZ5I%3D',\n",
       " 'page_number': 5,\n",
       " 'page_text': '<table><tr><th rowSpan=2>Model</th><th colSpan=3>SICK-E Datasets</th></tr><tr><th>standard</th><th>3-way</th><th>2-way</th></tr><tr><td>Majority</td><td>56.7</td><td>33.3</td><td>50.0</td></tr><tr><td>BERT (finetuned)</td><td>86.7</td><td>84.0</td><td>95.6</td></tr><tr><td>BERT (linear probing)</td><td>68.0</td><td>49.5</td><td>91.9</td></tr><tr><td>RoBERTa (linear probing)</td><td>72.6</td><td>49.4</td><td>91.1</td></tr><tr><td>BERT (AUTOPROMPT)</td><td>62.3</td><td>55.4</td><td>85.7</td></tr><tr><td>RoBERTa (AUTOPROMPT)</td><td>65.0</td><td>69.3</td><td>87.3</td></tr></table>\\nTable 2: Natural Language Inference performance on the SICK-E test set and variants. (Top) Baseline classi- fiers. (Bottom) Fill-in-the-blank MLMs.\\n(NLI). NLI is crucial in many tasks such as reading comprehension and commonsense reasoning (Bow- man et al., 2015), and it is used as a common bench- mark for language understanding.\\nSetup We use the entailment task from the SICK dataset (Marelli et al., 2014, SICK-E) which con- sists of around 10,000 pairs of human-annotated sentences labeled as entailment, contradiction, and neutral. The standard dataset is biased toward the neutral class which represent 56.7% of instances. We also experiment on an unbiased variant with 2-way classification of contradiction vs. entail- ment (2-way), as well as an unbiased 3-way clas- sification variant (3-way). The template used for AUTOPROMPT is provided in Table 3. We search over the following parameters: |Vcand| ∈ {10, 50}, |Vy| ∈ {1, 3, 5, 10}, |xtrig| ∈ [1, 5], and choose the best prompt according to development set accuracy.\\nResults Table 2 shows that AUTOPROMPT con- siderably outperforms the majority baseline in all experiments. For example, on the 2-way SICK-E dataset, AUTOPROMPT is comparable to a super- vised finetuned BERT. We also test linear probes— linear classifiers trained on top of frozen MLM representations with average pooling —and find AUTOPROMPT has comparable or higher accuracy, despite linear probes being susceptible to false pos- itives. Overall, these results demonstrate that both BERT and RoBERTa have some inherent knowl- edge of natural language inference.\\nWe also examine the efficacy of AUTOPROMPT in the low-data regime (using the same procedure as SST-2) on the unbiased 3-way SICK-E data. The results in Figure 2 show that AUTOPROMPT per- forms on par with finetuned BERT and significantly better than finetuned RoBERTa in low data settings.\\nMLMs Excel on Contradiction We find that the label tokens are more interpretable for con-\\ntradiction compared to entailment or neutral (ex- amples in Table 3). We investigate if this hurts the model performance on entailment and neutral classes.\\nWe measure the precision for each la- bel in the 3-way balanced SICK-E dataset. BERT achieves 74.9%, 54.4%, and 36.8% precision for contradiction, entailment, and neutral cases, respec- tively, while RoBERTa obtains 84.9%, 65.1%, and 57.3%. These results suggest that AUTOPROMPT may be more accurate for concepts that can be eas- ily expressed using natural label tokens.\\n5 Fact Retrieval\\nAn important question is whether pretrained MLMs know facts about real-world entities. The LAMA dataset (Petroni et al., 2019) evaluates this using cloze tests that consist of (sub, rel, obj) triples, e.g. (Obama, bornIn, Hawaii), and manually created prompts with missing objects, e.g. “Obama was born in [MASK].”. LPAQA (Jiang et al., 2020) ex- tends this idea by systematically creating prompts that are generated by mining Wikipedia, paraphras- ing, and crowdsourcing. In this section, we use the same cloze-style setup but automatically gener- ate prompts in order to better evaluate the factual knowledge of MLMs. We compare our approach against LAMA and LPAQA, which are explicitly designed for the task of fact retrieval.\\nSetup We reformulate fact retrieval by mapping (sub,rel,obj) triples to a prompt using the template “{sub}[T]. . . [T][P].”, where the trigger tokens are specific to the relation rel and the correct object obj is the label token. We use the original test set from LAMA (Petroni et al., 2019), henceforth Orig- inal. To collect training data for AUTOPROMPT, we gather at most 1000 facts for each of the 41 re- lations in LAMA from the T-REx dataset (ElSahar et al., 2018). For the relations that still have less than 1000 samples, we gather extra facts straight from Wikidata. We ensure that none of the T-REx triples are present in the test set, and we split the data 80-20 into train and development sets. More- over, because the collected T-REx data is from a slightly different distribution than the LAMA test set, we also consider a separate evaluation where we split the T-REx triples into a 60-20-20 train/dev/test split and evaluate on the test set. This T-REx dataset is used to measure the performance of our prompts when the train and test data is from the same distribution. '}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a single page of research paper file that will be indexed in Azure Cognitive Search\n",
    "documents[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Azure Forms Recognizer, Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "import os, json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes._search_index_client.SearchIndexClient at 0x228588fe390>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SDK client\n",
    "service_endpoint = os.environ[\"AZURE_COGNITIVE_SEARCH_ENDPOINT\"]  \n",
    "key = os.environ[\"AZURE_COGNITIVE_SEARCH_KEY\"]\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = \"research-paper-blob-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " research-paper-blob-index created\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(name=\"page_number\", type=SearchFieldDataType.Int64),\n",
    "    SimpleField(name=\"file_path\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"file_name\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"page_text\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"defualt\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"file_name\"),\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"page_text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 179 documents\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(result)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "text_model = os.environ['TEXT_DAVINCI_NAME']\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential # for exponential backoff\n",
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=30), stop=stop_after_attempt(10))\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\"): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieved_page</th>\n",
       "      <th>file_name</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Translation en-es\\nInstruction Only\\nIn-contex...</td>\n",
       "      <td>LLMs_are_Human-Level_Prompt_Engineers.pdf</td>\n",
       "      <td>[-0.028809441, 0.010793041, 0.017176475, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p(y|xprompt) = 3 p([MASK] = w|xprompt)\\nHoweve...</td>\n",
       "      <td>AutoPrompt_Eliciting_Knowledge_From_LanguageMo...</td>\n",
       "      <td>[-0.03455013, -0.008062171, -0.0059134425, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arXiv:2211.01910v1 [cs.LG] 3 Nov 2022\\nLARGE L...</td>\n",
       "      <td>LLMs_are_Human-Level_Prompt_Engineers.pdf</td>\n",
       "      <td>[-0.023766506, -0.004874184, 0.008511266, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD\\nLarge models...</td>\n",
       "      <td>LLMs_are_Human-Level_Prompt_Engineers.pdf</td>\n",
       "      <td>[-0.023554025, 0.004547, 0.006089732, -0.00838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Original Input Cinp a real joy.\\nAUTOPROMPT Ip...</td>\n",
       "      <td>AutoPrompt_Eliciting_Knowledge_From_LanguageMo...</td>\n",
       "      <td>[-0.028821949, -0.01375198, 0.005047462, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tianyu Gao, Adam Fisch, and Danqi Chen. Making...</td>\n",
       "      <td>LLMs_are_Human-Level_Prompt_Engineers.pdf</td>\n",
       "      <td>[-0.013933532, 0.0027675957, 0.015302562, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>how to write the chain of thought annotations ...</td>\n",
       "      <td>Chain-of-Thought_Prompting_Elicits_Reasoning_i...</td>\n",
       "      <td>[-0.02224152, 0.003555452, -0.002306708, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Task NumerSense\\nPrompt\\nGenerate some numeric...</td>\n",
       "      <td>Generated_Knowledge_Prompting_for_Commonsense_...</td>\n",
       "      <td>[-0.0053810575, 0.010437038, 0.028689751, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "      <td>AutoPrompt_Eliciting_Knowledge_From_LanguageMo...</td>\n",
       "      <td>[-0.017496426, 0.0055379565, 0.0054711495, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Question\\nTracy used a piece of wire 4 feet lo...</td>\n",
       "      <td>Chain-of-Thought_Prompting_Elicits_Reasoning_i...</td>\n",
       "      <td>[0.007523509, 0.012140825, 0.0124328025, -0.03...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      retrieved_page  \\\n",
       "0  Translation en-es\\nInstruction Only\\nIn-contex...   \n",
       "1  p(y|xprompt) = 3 p([MASK] = w|xprompt)\\nHoweve...   \n",
       "2  arXiv:2211.01910v1 [cs.LG] 3 Nov 2022\\nLARGE L...   \n",
       "3  A PROMPT ENGINEERING IN THE WILD\\nLarge models...   \n",
       "4  Original Input Cinp a real joy.\\nAUTOPROMPT Ip...   \n",
       "5  Tianyu Gao, Adam Fisch, and Danqi Chen. Making...   \n",
       "6  how to write the chain of thought annotations ...   \n",
       "7  Task NumerSense\\nPrompt\\nGenerate some numeric...   \n",
       "8  AUTOPROMPT: Eliciting Knowledge from Language ...   \n",
       "9  Question\\nTracy used a piece of wire 4 feet lo...   \n",
       "\n",
       "                                           file_name  \\\n",
       "0          LLMs_are_Human-Level_Prompt_Engineers.pdf   \n",
       "1  AutoPrompt_Eliciting_Knowledge_From_LanguageMo...   \n",
       "2          LLMs_are_Human-Level_Prompt_Engineers.pdf   \n",
       "3          LLMs_are_Human-Level_Prompt_Engineers.pdf   \n",
       "4  AutoPrompt_Eliciting_Knowledge_From_LanguageMo...   \n",
       "5          LLMs_are_Human-Level_Prompt_Engineers.pdf   \n",
       "6  Chain-of-Thought_Prompting_Elicits_Reasoning_i...   \n",
       "7  Generated_Knowledge_Prompting_for_Commonsense_...   \n",
       "8  AutoPrompt_Eliciting_Knowledge_From_LanguageMo...   \n",
       "9  Chain-of-Thought_Prompting_Elicits_Reasoning_i...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.028809441, 0.010793041, 0.017176475, -0.01...  \n",
       "1  [-0.03455013, -0.008062171, -0.0059134425, -0....  \n",
       "2  [-0.023766506, -0.004874184, 0.008511266, -0.0...  \n",
       "3  [-0.023554025, 0.004547, 0.006089732, -0.00838...  \n",
       "4  [-0.028821949, -0.01375198, 0.005047462, -0.01...  \n",
       "5  [-0.013933532, 0.0027675957, 0.015302562, -0.0...  \n",
       "6  [-0.02224152, 0.003555452, -0.002306708, -0.02...  \n",
       "7  [-0.0053810575, 0.010437038, 0.028689751, 0.00...  \n",
       "8  [-0.017496426, 0.0055379565, 0.0054711495, -0....  \n",
       "9  [0.007523509, 0.012140825, 0.0124328025, -0.03...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is automated prompt engineering?\"\n",
    "count = 10\n",
    "results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "page_chunks = []\n",
    "citations = []\n",
    "for result in results:\n",
    "    page_chunks.append(result['page_text'])\n",
    "    citations.append(result['file_name'])\n",
    "    \n",
    "embed_df = pd.DataFrame({\"retrieved_page\":page_chunks, \"file_name\":citations}) #datframe with document chunks and citations\n",
    "embed_df['embedding'] = embed_df[\"retrieved_page\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))\n",
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieved_page</th>\n",
       "      <th>file_name</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD\\nLarge models...</td>\n",
       "      <td>LLMs_are_Human-Level_Prompt_Engineers.pdf</td>\n",
       "      <td>[-0.023554025, 0.004547, 0.006089732, -0.00838...</td>\n",
       "      <td>0.885291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arXiv:2211.01910v1 [cs.LG] 3 Nov 2022\\nLARGE L...</td>\n",
       "      <td>LLMs_are_Human-Level_Prompt_Engineers.pdf</td>\n",
       "      <td>[-0.023766506, -0.004874184, 0.008511266, -0.0...</td>\n",
       "      <td>0.847408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "      <td>AutoPrompt_Eliciting_Knowledge_From_LanguageMo...</td>\n",
       "      <td>[-0.017496426, 0.0055379565, 0.0054711495, -0....</td>\n",
       "      <td>0.835019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      retrieved_page  \\\n",
       "0  A PROMPT ENGINEERING IN THE WILD\\nLarge models...   \n",
       "1  arXiv:2211.01910v1 [cs.LG] 3 Nov 2022\\nLARGE L...   \n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...   \n",
       "\n",
       "                                           file_name  \\\n",
       "0          LLMs_are_Human-Level_Prompt_Engineers.pdf   \n",
       "1          LLMs_are_Human-Level_Prompt_Engineers.pdf   \n",
       "2  AutoPrompt_Eliciting_Knowledge_From_LanguageMo...   \n",
       "\n",
       "                                           embedding  similarities  \n",
       "0  [-0.023554025, 0.004547, 0.006089732, -0.00838...      0.885291  \n",
       "1  [-0.023766506, -0.004874184, 0.008511266, -0.0...      0.847408  \n",
       "2  [-0.017496426, 0.0055379565, 0.0054711495, -0....      0.835019  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = get_embedding(query, engine=embedding_model)\n",
    "embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "top_results = (\n",
    "    embed_df.sort_values(\"similarities\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .head(3)\n",
    ")\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Prompt Engineering (APE) is a method proposed in the paper \"Large Language Prompt Engineers Models are Human-Level\" [LLMs_are_Human-Level_Prompt_Engineers.pdf] for automatic instruction generation and selection. APE treats the instruction as the \"program,\" optimized by searching over a pool of instruction candidates proposed by a large language model (LLM) in order to maximize a chosen score function. The goal of APE is to reduce the human effort involved in creating and validating effective instructions. APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "    You are a research assistant and can help summarize and answer questions on research papers.\n",
    "    Answer ONLY with the facts listed in the sources below. If there isn't enough information below, say you don't know. \n",
    "    Each source is a dictionary with file_name and information available in retrieved_page, always include the source name for each fact you use in the response. Use square brakets to reference the source, e.g. [info1.pdf]. Don't combine sources, list each source separately, e.g. [info1.pdf][info2.pdf].\n",
    "    Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question. \n",
    "    The question and sources are delimited by triple backticks.\n",
    "    Sourcers contain both text and tables. Tables in presented in HTML format. Parse the HTML step-by-step to extract the information you need.\n",
    "\n",
    "    Question: ```{query}``` \\n\n",
    "    Sources: ```{top_results[['retrieved_page', 'file_name']].to_dict('records')}``` \\n\n",
    "\n",
    "    Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_search(query, count=10):\n",
    "    results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "    page_chunks = []\n",
    "    citations = []\n",
    "    for result in results:\n",
    "        page_chunks.append(result['page_text'])\n",
    "        citations.append(result['file_name'])\n",
    "                \n",
    "    embed_df = pd.DataFrame({\"retrieved_page\":page_chunks, \"file_name\":citations}) #datframe with document chunks and citations\n",
    "        \n",
    "    #Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "    embed_df[\"embedding\"] = embed_df[\"retrieved_page\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))\n",
    "\n",
    "    query_embedding = get_embedding(query, engine=embedding_model)\n",
    "    embed_df[\"similarities\"] = embed_df[\"embedding\"].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "    top_results = (\n",
    "        embed_df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        You are a research assistant and can help summarize and answer questions on research papers.\n",
    "        Answer ONLY with the facts listed in the sources below. If there isn't enough information below, say you don't know. \n",
    "        Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question. \n",
    "        Each source is a dictionary with actual information available in retrieved_page key and source name is the file_name key. Always include the source name separately at end of the response in a new line. Use square brakets to reference the source name, e.g. [Research_Paper.pdf]. Don't combine sources, list each source separately, e.g. [ResarchPaper1.pdf][Research_Paper.pdf].\n",
    "        The question and sources are delimited by triple backticks.\n",
    "        Sourcers contain both text and tables. Tables in presented in HTML format. Parse the HTML step-by-step to extract the information you need.\n",
    "\n",
    "        Question: ```{query}``` \\n\n",
    "        Sources: ```{top_results[['retrieved_page', 'file_name']].to_dict('records')}``` \\n\n",
    "\n",
    "        Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conclusion from the paper \"AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts\" is that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning. The paper shows that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. The prompts generated by AUTOPROMPT elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and MLMs can be used as relation extractors more effectively than supervised relation extraction models. [AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf]\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"what is the conclusion from the paper - AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts?\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Knowledge Generation methods include:\n",
      "- Generating numerical facts about objects [Task NumerSense][Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf]\n",
      "- Generating knowledge about concepts in the input [Task Prompt CSQA][Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf]\n",
      "- Generating knowledge about the input [Task QASC][Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf]\n",
      "- Providing explanations or definitions for given inputs [Task Prompt CSQA2][Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf]\n",
      "\n",
      "[Task NumerSense][Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf]\n",
      "[Task Prompt CSQA][Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf]\n",
      "[Task QASC][Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf]\n",
      "[Task Prompt CSQA2][Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf]\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"What are the examples of Knowledge Generation methods?\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is only one research paper that mentions automated prompt engineering, and it is \"LLMs are Human-Level Prompt Engineers\" [LLMs_are_Human-Level_Prompt_Engineers.pdf].\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"List all the research papers that have info about automated prompt engineering.\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT Prompting stands for Chain of Thought Prompting. It is a type of prompting that enables length generalization to longer inference examples on two symbolic manipulation tasks and elicits reasoning in large language models (LLMs). It is an emergent ability of model scale and does not positively impact performance until used with a model of sufficient scale. [Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf]\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"Expand CoT Prompting and explain what it is.\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Natural Language Inference (NLI) performance on the SICK-E test set and variants is presented in Table 2 of [AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf]. The table shows the performance of various models on the standard, 3-way, and 2-way datasets of SICK-E. The models include Majority, BERT (finetuned), BERT (linear probing), RoBERTa (linear probing), BERT (AUTOPROMPT), and RoBERTa (AUTOPROMPT). The table shows that BERT (finetuned) has the highest performance on all three datasets, followed by RoBERTa (AUTOPROMPT) on the 3-way dataset and BERT (linear probing) on the 2-way dataset. The observations are as follows:\n",
      "\n",
      "| Model                   | Standard | 3-way | 2-way |\n",
      "|-------------------------|----------|-------|-------|\n",
      "| Majority                | 56.7     | 33.3  | 50.0  |\n",
      "| BERT (finetuned)        | 86.7     | 84.0  | 95.6  |\n",
      "| BERT (linear probing)   | 68.0     | 49.5  | 91.9  |\n",
      "| RoBERTa (linear probing)| 72.6     | 49.4  | 91.1  |\n",
      "| BERT (AUTOPROMPT)       | 62.3     | 55.4  | 85.7  |\n",
      "| RoBERTa (AUTOPROMPT)    | 65.0     | 69.3  | 87.3  |\n",
      "\n",
      "[AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf]\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"I am looking for Natural Language Inference performance on the SICK-E test set and variants. Could you list the observations in a tabular format?\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEIR stands for \"Benchmarking Indexing and Retrieval\" and is a dataset for low-resource retrieval tasks. Table 2 in [Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf] shows the low resource tasks from BEIR and their results.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"what is BEIR? Show low resource tasks from BEIR in a table.\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
