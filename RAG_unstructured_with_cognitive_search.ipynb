{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "text_model = os.environ['TEXT_DAVINCI_NAME']\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- raw data\n",
    "RAW_DATA_FOLDER= './data/unstructured/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = './data/unstructured/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_single_file(file_name: str):\n",
    "    not_completed = True\n",
    "    while not_completed:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            not_completed=False\n",
    "    result = poller.result()\n",
    "    return get_page_content(file_name, result)\n",
    "\n",
    "def extract_files( folder_name: str, destination_folder_name: str):\n",
    "    os.makedirs(destination_folder_name, exist_ok=True)\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file[-3:].upper() in ['PDF','JPG','PNG']:\n",
    "            print('Processing file:', file, end='')\n",
    "        \n",
    "            page_content = extract_local_single_file(os.path.join(folder_name, file))\n",
    "            output_file = os.path.join(destination_folder_name, file[:-3] +'json')\n",
    "            print(f'  write output to {output_file}')\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(json.dumps(page_content))\n",
    "\n",
    "\n",
    "def get_page_content(file_name:str, result):\n",
    "    page_content = []\n",
    "    for page in result.pages:\n",
    "        all_lines_content = []\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            all_lines_content.append(' '.join([word.content for word in line.get_words()]))\n",
    "        page_content.append({'page_number':page.page_number, \n",
    "                                'page_content':' '.join(all_lines_content)})\n",
    "    return {'filename':file_name, 'content':page_content}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf  write output to ./data/unstructured/extracted\\AutoPrompt_Eliciting_Knowledge_From_LanguageModels.json\n",
      "Processing file: Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf  write output to ./data/unstructured/extracted\\Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.json\n",
      "Processing file: Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf  write output to ./data/unstructured/extracted\\Generated_Knowledge_Prompting_for_Commonsense_Reasoning.json\n",
      "Processing file: LLMs_are_Human-Level_Prompt_Engineers.pdf  write output to ./data/unstructured/extracted\\LLMs_are_Human-Level_Prompt_Engineers.json\n",
      "Processing file: Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf  write output to ./data/unstructured/extracted\\Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.json\n",
      "Processing file: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf  write output to ./data/unstructured/extracted\\Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.json\n",
      "Processing file: Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf  write output to ./data/unstructured/extracted\\Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.json\n",
      "Processing file: Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.pdf  write output to ./data/unstructured/extracted\\Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.json\n"
     ]
    }
   ],
   "source": [
    "extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for file in os.listdir(EXTRACTED_DATA_FOLDER):\n",
    "    with open(os.path.join(EXTRACTED_DATA_FOLDER, file)) as f:\n",
    "        page_content= json.loads(f.read())\n",
    "    documents.extend([{'document_id':page_content['filename'].split('\\\\')[-1].split('.')[0] + '-' + str(page['page_number']),\\\n",
    "                        'document_name':page_content['filename'].split('\\\\')[-1],\\\n",
    "                        'file_path':page_content['filename'],\\\n",
    "                        'page_number': page['page_number'],\\\n",
    "                        'page_text': page['page_content'] } \\\n",
    "                        for page in page_content['content'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': 'AutoPrompt_Eliciting_Knowledge_From_LanguageModels-1',\n",
       " 'document_name': 'AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf',\n",
       " 'file_path': './data/unstructured/raw\\\\AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf',\n",
       " 'page_number': 1,\n",
       " 'page_text': 'AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts Taylor Shin*♦ Yasaman Razeghi∗♦ Eric Wallace♠ ♦University of California, Irvine Robert L. Logan IV∗♦ Sameer Singh♦ ♠University of California, Berkeley {tshin1, yrazeghi, rlogan, sameer}@uci.edu ericwallace@berkeley.edu arXiv:2010.15980v2 [cs.CL] 7 Nov 2020 Abstract The remarkable success of pretrained lan- guage models has motivated the study of what kinds of knowledge these models learn dur- ing pretraining. Reformulating tasks as fill- in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suit- able prompts. To address this, we develop AUTOPROMPT, an automated method to cre- ate prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTO- PROMPT, we show that masked language mod- els (MLMs) have an inherent capability to perform sentiment analysis and natural lan- guage inference without additional parame- ters or finetuning, sometimes achieving per- formance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowl- edge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that au- tomatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a re- placement for finetuning. 1 Introduction Pretrained language models (LMs) have had ex- ceptional success when adapted to downstream tasks via finetuning (Peters et al., 2018; Devlin et al., 2019). Although it is clear that pretrain- ing improves accuracy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning * First three authors contributed equally. process. How can we directly evaluate the knowl- edge present in pretrained LMs, be it linguistic, factual, commonsense, or task-specific? Numerous techniques have been proposed to elicit such knowledge by analyzing pretrained LMs’ internal representations. A common strategy is to use probing classifiers—shallow classifiers that predict certain attributes using an LMs’ representa- tions as features (Conneau et al., 2018; Liu et al., 2019). However, probing classifiers require ad- ditional learned parameters and are thus suscep- tible to false positives; high probing accuracy is not a sufficient condition to conclude that an LM contains a certain piece of knowledge (Hewitt and Liang, 2019; Voita and Titov, 2020). Attention visualization, another common technique, has a similar failure mode: attention scores may be corre- lated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Both probing and attention visu- alizations also struggle to evaluate knowledge that cannot be represented as simple token- or sequence- level classification tasks. A more direct approach for eliciting knowledge from these models, since they are language models after all, is prompting, i.e. converting tasks into a language model format. For example, Radford et al. (2019) frame summarization as a language modeling task by appending “TL;DR:” to the end of an article and then generating from an LM. Sim- ilarly, Petroni et al. (2019) manually reformulate a knowledge base completion task as a cloze test (i.e., a fill-in-the-blank problem). Compared to existing model analysis methods, prompting is non- invasive: it does not introduce large amounts of additional parameters or require direct inspection of a model’s representations. Thus prompting pro- 1'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a single page of research paper file that will be indexed in Azure Cognitive Search\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes._search_index_client.SearchIndexClient at 0x25edce808b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SDK client\n",
    "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = \"research-paper-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " research-paper-index created\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"document_id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(name=\"page_number\", type=SearchFieldDataType.Int64),\n",
    "    SimpleField(name=\"file_path\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"page_text\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"document_id\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"document_name\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"page_text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 179 documents\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is automated prompt engineering?\"\n",
    "count = 10\n",
    "results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "page_chunks = []\n",
    "for result in results:\n",
    "    page_chunks.append(result['page_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 LARGE LA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question Tracy used a piece of wire 4 feet lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Translation en-es Instruction Only In-context ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Original Input Cinp a real joy. AUTOPROMPT Ipr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p(y|xprompt) = 3 p([MASK] = w|xprompt) However...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Task Prompt CSQA2 Generate some knowledge abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Task NumerSense Prompt Generate some numerical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              chunks\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...\n",
       "1  arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 LARGE LA...\n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...\n",
       "3  Question Tracy used a piece of wire 4 feet lon...\n",
       "4  Translation en-es Instruction Only In-context ...\n",
       "5  Original Input Cinp a real joy. AUTOPROMPT Ipr...\n",
       "6  p(y|xprompt) = 3 p([MASK] = w|xprompt) However...\n",
       "7  Table 24: Few-shot exemplars for full chain of...\n",
       "8  Task Prompt CSQA2 Generate some knowledge abou...\n",
       "9  Task NumerSense Prompt Generate some numerical..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df = pd.DataFrame(page_chunks, columns = [\"chunks\"]) #datframe with document chunks\n",
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            count+=1\n",
    "            #print(f'RateLimitError Count: {count}')\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\"): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RateLimitError Count: 1\n",
      "RateLimitError Count: 2\n",
      "RateLimitError Count: 3\n",
      "RateLimitError Count: 4\n",
      "RateLimitError Count: 5\n",
      "RateLimitError Count: 1\n",
      "RateLimitError Count: 2\n",
      "RateLimitError Count: 3\n",
      "RateLimitError Count: 4\n",
      "RateLimitError Count: 5\n",
      "RateLimitError Count: 1\n",
      "RateLimitError Count: 2\n",
      "RateLimitError Count: 3\n",
      "RateLimitError Count: 4\n",
      "RateLimitError Count: 5\n",
      "RateLimitError Count: 1\n",
      "RateLimitError Count: 2\n",
      "RateLimitError Count: 3\n",
      "RateLimitError Count: 4\n",
      "RateLimitError Count: 5\n"
     ]
    }
   ],
   "source": [
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embedding'] = embed_df[\"chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "      <td>[-0.024585616, -0.00022426769, 0.00631925, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 LARGE LA...</td>\n",
       "      <td>[-0.021359596, -0.007799931, -0.00046711107, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "      <td>[-0.01975671, 0.0031453124, 0.0019787818, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question Tracy used a piece of wire 4 feet lon...</td>\n",
       "      <td>[0.007719166, 0.007380606, 0.0143549405, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Translation en-es Instruction Only In-context ...</td>\n",
       "      <td>[-0.028066656, 0.0065712705, 0.016445307, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Original Input Cinp a real joy. AUTOPROMPT Ipr...</td>\n",
       "      <td>[-0.031109842, -0.018427676, 0.0044843014, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p(y|xprompt) = 3 p([MASK] = w|xprompt) However...</td>\n",
       "      <td>[-0.03274989, -0.011971078, -0.0063749263, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "      <td>[0.013763685, 0.012710237, 0.03043296, -0.0045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Task Prompt CSQA2 Generate some knowledge abou...</td>\n",
       "      <td>[0.017677374, 0.031847112, 0.037832364, -0.007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Task NumerSense Prompt Generate some numerical...</td>\n",
       "      <td>[-0.0027567996, 0.012855793, 0.03290082, -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              chunks  \\\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...   \n",
       "1  arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 LARGE LA...   \n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...   \n",
       "3  Question Tracy used a piece of wire 4 feet lon...   \n",
       "4  Translation en-es Instruction Only In-context ...   \n",
       "5  Original Input Cinp a real joy. AUTOPROMPT Ipr...   \n",
       "6  p(y|xprompt) = 3 p([MASK] = w|xprompt) However...   \n",
       "7  Table 24: Few-shot exemplars for full chain of...   \n",
       "8  Task Prompt CSQA2 Generate some knowledge abou...   \n",
       "9  Task NumerSense Prompt Generate some numerical...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.024585616, -0.00022426769, 0.00631925, -0....  \n",
       "1  [-0.021359596, -0.007799931, -0.00046711107, -...  \n",
       "2  [-0.01975671, 0.0031453124, 0.0019787818, -0.0...  \n",
       "3  [0.007719166, 0.007380606, 0.0143549405, -0.03...  \n",
       "4  [-0.028066656, 0.0065712705, 0.016445307, -0.0...  \n",
       "5  [-0.031109842, -0.018427676, 0.0044843014, -0....  \n",
       "6  [-0.03274989, -0.011971078, -0.0063749263, -0....  \n",
       "7  [0.013763685, 0.012710237, 0.03043296, -0.0045...  \n",
       "8  [0.017677374, 0.031847112, 0.037832364, -0.007...  \n",
       "9  [-0.0027567996, 0.012855793, 0.03290082, -0.00...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "      <td>[-0.024585616, -0.00022426769, 0.00631925, -0....</td>\n",
       "      <td>0.890530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 LARGE LA...</td>\n",
       "      <td>[-0.021359596, -0.007799931, -0.00046711107, -...</td>\n",
       "      <td>0.866601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "      <td>[-0.01975671, 0.0031453124, 0.0019787818, -0.0...</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              chunks  \\\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...   \n",
       "1  arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 LARGE LA...   \n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...   \n",
       "\n",
       "                                           embedding  similarities  \n",
       "0  [-0.024585616, -0.00022426769, 0.00631925, -0....      0.890530  \n",
       "1  [-0.021359596, -0.007799931, -0.00046711107, -...      0.866601  \n",
       "2  [-0.01975671, 0.0031453124, 0.0019787818, -0.0...      0.842000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = get_embedding(query, engine=embedding_model)\n",
    "embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "top_results = (\n",
    "    embed_df.sort_values(\"similarities\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .head(3)\n",
    ")\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
      "Your task is to extract key pieces of information from that list based on the user query and phrase that as answer. \n",
      "\n",
      "User Query: ```What is automated prompt engineering?```\n",
      "List of Extracted Pages: ```['A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: • https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ • https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ • https://news.ycombinator.com/item?id=32943224 • https://promptomania.com/stable-diffusion-prompt-builder/ • https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15', 'arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou∗,1,2 Andrei Ioan Muresanu∗,2,3 Ziwen Han∗,1,2 Keiran Paster1,2 Silviu Pitis1,2 Harris Chan1,2 Jimmy Ba1,2 1University of Toronto 2Vector Institute 3University of Waterloo ABSTRACT By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. 1 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called “large language models” (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem * Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https://github.com/keirp/automatic_prompt_engineer. 1', 'AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts Taylor Shin*♦ Yasaman Razeghi∗♦ Eric Wallace♠ ♦University of California, Irvine Robert L. Logan IV∗♦ Sameer Singh♦ ♠University of California, Berkeley {tshin1, yrazeghi, rlogan, sameer}@uci.edu ericwallace@berkeley.edu arXiv:2010.15980v2 [cs.CL] 7 Nov 2020 Abstract The remarkable success of pretrained lan- guage models has motivated the study of what kinds of knowledge these models learn dur- ing pretraining. Reformulating tasks as fill- in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suit- able prompts. To address this, we develop AUTOPROMPT, an automated method to cre- ate prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTO- PROMPT, we show that masked language mod- els (MLMs) have an inherent capability to perform sentiment analysis and natural lan- guage inference without additional parame- ters or finetuning, sometimes achieving per- formance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowl- edge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that au- tomatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a re- placement for finetuning. 1 Introduction Pretrained language models (LMs) have had ex- ceptional success when adapted to downstream tasks via finetuning (Peters et al., 2018; Devlin et al., 2019). Although it is clear that pretrain- ing improves accuracy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning * First three authors contributed equally. process. How can we directly evaluate the knowl- edge present in pretrained LMs, be it linguistic, factual, commonsense, or task-specific? Numerous techniques have been proposed to elicit such knowledge by analyzing pretrained LMs’ internal representations. A common strategy is to use probing classifiers—shallow classifiers that predict certain attributes using an LMs’ representa- tions as features (Conneau et al., 2018; Liu et al., 2019). However, probing classifiers require ad- ditional learned parameters and are thus suscep- tible to false positives; high probing accuracy is not a sufficient condition to conclude that an LM contains a certain piece of knowledge (Hewitt and Liang, 2019; Voita and Titov, 2020). Attention visualization, another common technique, has a similar failure mode: attention scores may be corre- lated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Both probing and attention visu- alizations also struggle to evaluate knowledge that cannot be represented as simple token- or sequence- level classification tasks. A more direct approach for eliciting knowledge from these models, since they are language models after all, is prompting, i.e. converting tasks into a language model format. For example, Radford et al. (2019) frame summarization as a language modeling task by appending “TL;DR:” to the end of an article and then generating from an LM. Sim- ilarly, Petroni et al. (2019) manually reformulate a knowledge base completion task as a cloze test (i.e., a fill-in-the-blank problem). Compared to existing model analysis methods, prompting is non- invasive: it does not introduce large amounts of additional parameters or require direct inspection of a model’s representations. Thus prompting pro- 1']```\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "User Query: ```{query}```\n",
    "List of Extracted Pages: ```{top_results['chunks'].to_list()}```\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated prompt engineering is a method for automatically generating and selecting natural language instructions to steer large language models (LLMs) towards desired behaviors. It involves searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. A number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Some examples include: https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/, https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/, https://news.ycombinator.com/item?id=32943224, https://promptomania.com/stable-diffusion-prompt-builder/, and https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_search(query, count=10):\n",
    "    results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "    page_chunks = []\n",
    "    for result in results:\n",
    "        page_chunks.append(result['page_text'])\n",
    "        \n",
    "    #Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "    embed_df['embedding'] = embed_df[\"chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))\n",
    "\n",
    "    query_embedding = get_embedding(query, engine=embedding_model)\n",
    "    embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "    top_results = (\n",
    "        embed_df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "    Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "    User Query: ```{query}```\n",
    "    List of Extracted Pages: ```{top_results['chunks'].to_list()}```\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated prompt engineering involves using natural language instructions to steer large language models (LLMs) towards desired behaviors. This is achieved through the use of algorithms such as Automatic Prompt Engineer (APE) which generates and selects instructions automatically by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. The quality of the selected instruction is evaluated by evaluating the zero-shot performance of another LLM following the selected instruction. A number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed to assist in finding the right prompt. These include resources such as https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/, https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/, https://news.ycombinator.com/item?id=32943224, https://promptomania.com/stable-diffusion-prompt-builder/, and https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"How does automated prompt engineering work?\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tuning refers to the process of finding the right prompt for natural language interfaces, including models for text generation and image synthesis. It can be difficult for humans to find the right prompt, so a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. AUTOPROMPT is an automated method for generating prompts for any task, based on a gradient-guided search. It creates a prompt by combining the original task inputs with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy. The LM predictions for the prompt are converted to class probabilities by marginalizing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"what is prompt tuning?\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaiworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
